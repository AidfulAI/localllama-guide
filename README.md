Welcome to the Localllama Guide. The name is aligned to the Subreddit [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/), to my knowledge, the best place to discuss and get the latest information and insights about running generative AI and especially text generating Large Language Models (LLMs) locally on your own hardware.

This resource is dedicated to providing information to get started and look-up information. In contrast to the [r/LocalLLaMA wiki](https://www.reddit.com/r/LocalLLaMA/wiki/index/), which contains a lot of valuable information about models, you'll find a list of available software options, alongside recommendations for the necessary hardware to get started in this guide. It is not meant to be a replacement of the r/LocalLLaMA Subreddit or their wiki, but as a place which collects and curates information to get you started with running LLMs locally.

## Available Tools

Several developers and organizations created tools which make LLMs accessible for local deployment. Below are some of the notable options:

- **[JanFramework](https://twitter.com/janframework)**: User friendly UI with a limited model zoo.
- **[Ollama](Ollama)**: Command line tool which is super easy to use.
- **[LMStudioAI](https://twitter.com/LMStudioAI)**: Allows to browse and download models in different quantizations and even allows to partially ofload layers to a GPU for large models.
- **GPT4All by [Nomic AI](https://twitter.com/nomic_ai)**: ChatGPT clone which even can integrate GPT-4 via API-key. Supports chatting with your documents
- **Oobabooga**: Most flexible, but also complex and with that difficult to get started option.

## Hardware Requirements

To run these LLMs locally, the hardware requirements can vary based on the model's complexity and the desired response time.

- **Mac Users**: Owners of recent Macs with M series chips are generally well-equipped to run local LLMs. A larger system memory allows you to run larger models.
- **PC Users**: Those on other platforms will likely need a powerful GPU to ensure smooth operation. NVIDIA GPUs are recommended due to their widespread support and high memory capacity, which is crucial for handling the demands of large models. Without a GPU, only small models run in a reasonable time.

## Future Outlook

The landscape of local AI computation is rapidly evolving. With every major chip manufacturer focusing on enhancing the capabilities of local AI processing, we can anticipate significant improvements in the efficiency and accessibility of running LLMs locally in the coming years.

Stay tuned to this page for updates on new developments and more insights into making the most of local LLMs.
